{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Diffusion Models</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion models are a class of latent variable generative models capable of generating samples that are distributed \"similarly\" as the training dataset. Learning is achieved by applying a series of forward Diffusion models apply progressively more noise to the data during a sequence of transformation steps. A neural network can be trained to learn how to generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diffusion model learns how do generate samples that are \"similar\" to the data it trains on by applying a series of forward and reverse diffusion processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{0}\\overset{forward}{\\underset{reverse}\\rightleftarrows} x_{1:T}$<br>\n",
    "$\\text{where}$<br>\n",
    "$x_{1:T}= x_{0}\\rightarrow x_{1}\\rightarrow\\dots\\rightarrow x_{T}$<br>\n",
    "$\\text{and}$<br>\n",
    "$x_{t+1}= x_{t-1}$<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Forward Diffusion Process</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A forward diffusion process of $T\\in(0,\\inf)$ sequential transformations, $\\xrightarrow{\\text{forward}}$, to a random variable $x_{0}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{1:T}= x_{0}\\rightarrow x_{1}\\rightarrow\\dots\\rightarrow x_{T}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each latent variable $x_{t}\\in x_{1:T}$ consists of the sum of noise and the value of $x_{t-1}$.<br>\n",
    "The joint distribution $q(x_{1:T}|x_{0})$ of latent variables $x_{1:T}$ given the original manifest variable $x_{0}$ is memoryless.<br>\n",
    "$q(x_{t:T}|x_{0})=\\prod\\limits_{t=1}^{T}{q(x_{t}|x_{t-1})}\\quad\\quad\\quad\\quad\\text{(Markov Property)}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H4>Continuous Forward Diffusion Process Using Diagonal Gaussian</H4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at point $x_{0}\\sim\\ q$, where $q$ is a probability distribution that will be learned, add noise to the original data in the following fashion:<br>\n",
    "$x_{t}=\\sqrt{1-\\beta_{t}}x_{t-1}+\\sqrt{\\beta_{t}}z_{t}$<br>\n",
    "$\\text{where}$<br>\n",
    "$z_{1},\\dots,z_{T}\\text{ are I.I.D. }\\mathcal{N}(O,I))$<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let $q(x_{t}|x_{t-1})\\sim\\mathcal{N}(x_{t};\\sqrt{1-\\beta}x_{t-1},\\beta_{t}I)\\quad\\quad\\quad\\quad\\quad\\quad\\quad$<br>\n",
    "$\\text{where:}\\quad\\quad\\quad\\beta_{t}\\text{is the variance at time t}$<br>\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad \\beta_{1}\\lt\\beta_{2}\\lt\\dots<\\beta_{T}$<br>\n",
    "$\\quad\\quad\\quad\\quad\\quad\\quad \\beta_{t}\\in(0, 1)$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axiom: $q(x_{T}|x_{0})\\rightarrow\\mathcal{N}(0,I)$ as $T\\rightarrow\\inf$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's constrain ourselves to a versy small $B_{t}\\ll1$. There are claims (refs??) that using a small step size makes the reverse process of learning the diffusion process more stable.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Continous Reverse Diffusion Process Using Diagonal Gaussian Kernel</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a fully developed latent space, we can train a neural network to learn the parameters needed to generate similarly distributed samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Feller's \"On the Theory of Stochastic Processes, with Particular Reference to Applications\" (ref??), claim states that diffusion models can and will converge to an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{0}\\leftarrow\\dots\\leftarrow x_{T-1}\\leftarrow x_{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_\\theta(x_{0:T})=p(x_{T})\\prod\\limits_{t=1}^{T}p_{\\theta}(x_{t-1}|x_{t})$<br>\n",
    "where<br>\n",
    "$p_{\\theta}(x_{T})\\sim\\mathcal{N}(x_{T}; 0, I)$<br>\n",
    "and<br>\n",
    "$p_\\theta(x{t-1}|x_{t})\\sim\\mathcal{N}(x_{t-1};\\mu_\\theta(x_{t}, t), \\Sigma_\\theta(x_{t}, t))$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal distribution $p_{\\theta}(x_{0})$ requires integration over all 1:T latent dimensions which becomes intractable quickly.\n",
    "$p_{\\theta}(x_{0})=\\int p_{\\theta}(x_{0:T}dx_{1:T}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to the dimensionallity problem is to use a variational lower bound.<br>\n",
    "$log\\ p_\\theta(x)\\ge\\text{variational lower bound (ELBO)}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$log\\ p_\\theta(x_{0})\\ge\\mathbb{E}_{q(x_{1:T}|x_{0})}[log\\ p_{\\theta}(x_{0}|x_{1:T})] - D_{KL}(q(x_{1:T}|x_{0})\\ ||\\ p_{\\theta}(x_{1:T}))$<br>\n",
    "$\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{q(x_{1:T}|x_{0})}[log\\ p_{\\theta}(x_{0}|x_{1:T})] - \\mathbb{E}_{q(x_{1:T}|x_{0})}\\left[\\frac{q(x_{1:T}|x_{0})}{p_{\\theta}(x_{1:T})}\\right]$<br>\n",
    "$\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{q(x_{1:T}|x_{0})}\\left[log\\ p_{\\theta}(x_{0}|x_{1:T})+log\\ \\frac{p_{\\theta}(x_{1:t})}{q(x_{1:T}|x_{0})} \\right]$<br>\n",
    "$\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{q(x_{1:T}|x_{0})}\\left[log\\ p_{\\theta}(x_{0})+\\sum\\limits_{t\\ge1}\\ log\\frac{p_{\\theta}(x_{t-1}|x_{t})}{q(x_{t}|x{t-1})}\\right]$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axiom: This variational lower bound (aka the Evidence Lower Bound - ELBO) has high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sohl-Dickstein demonstrate the derivation of a reduced objective in [1, Appendix B].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>References</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sohl-Dickstein J. et. al., \"Deep Unsupervised Learning using\n",
    "Nonequilibrium Thermodynamics\" arXiv:1503.03585v, rev. Nov. 18, 2015.\n",
    "2. Feller W., \"On the Theory of Stochastic Processes, with Particular Reference to Applications\", "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
